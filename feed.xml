<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://pair.stanford.edu/feed.xml" rel="self" type="application/atom+xml" /><link href="http://pair.stanford.edu/" rel="alternate" type="text/html" /><updated>2019-01-15T21:01:10-08:00</updated><id>http://pair.stanford.edu/</id><title type="html">Stanford PAIR Website</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title><link href="http://pair.stanford.edu/publications/2019/01/15/wang-arxiv19-6dofpose.html" rel="alternate" type="text/html" title="DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion" /><published>2019-01-15T00:00:00-08:00</published><updated>2019-01-15T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/01/15/wang-arxiv19-6dofpose</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/01/15/wang-arxiv19-6dofpose.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal.html" rel="alternate" type="text/html" title="Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks" /><published>2018-10-25T00:00:00-07:00</published><updated>2018-10-25T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal.html">&lt;p&gt;Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation</title><link href="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html" rel="alternate" type="text/html" title="RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html">&lt;p&gt;Imitation Learning has empowered recent advances in learning robotic manipulation tasks by addressing shortcomings of Reinforcement Learning such as exploration and reward specification. However, research in this area has been limited to modest-sized datasets due to the difficulty of collecting large quantities of task demonstrations through existing mechanisms. This work introduces ROBOTURK to address this challenge. ROBOTURK is a crowdsourcing platform for high quality 6-DoF trajectory based teleoperation through the use of widely available mobile devices (e.g. iPhone). We evaluate ROBOTURK on three manipulation tasks of varying timescales (15-120s) and observe that our user interface is statistically similar to special purpose hardware such as virtual reality controllers in terms of task completion times. Furthermore, we observe that poor network conditions, such as low bandwidth and high delay links, do not substantially affect the remote users’ ability to perform task demonstrations successfully on ROBOTURK. Lastly, we demonstrate the efficacy of ROBOTURK through the collection of a pilot dataset; using ROBOTURK, we collected 137.5 hours of manipulation data from remote workers, amounting to over 2200 successful task demonstrations in 22 hours of total system usage. We show that the data obtained through ROBOTURK enables policy learning on multi-step manipulation tasks with sparse rewards and that using larger quantities of demonstrations during policy learning provides benefits in terms of both learning consistency and final performance.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark</title><link href="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html" rel="alternate" type="text/html" title="SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html">&lt;p&gt;Reproducibility has been a significant challenge in deep reinforcement learning and robotics research. Open-source frameworks and standardized benchmarks can serve an integral role in rigorous evaluation and reproducible research. We introduce SURREAL, an open-source scalable framework that supports stateof-the-art distributed reinforcement learning algorithms. We design a principled distributed learning formulation that accommodates both on-policy and off-policy learning. We demonstrate that SURREAL algorithms outperform existing open-source implementations in both agent performance and learning efficiency. We also introduce SURREAL Robotics Suite, an accessible set of benchmarking tasks in physical simulation for reproducible robot manipulation research. We provide extensive evaluations of SURREAL algorithms and establish strong baseline results.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter</title><link href="http://pair.stanford.edu/publications/2018/09/15/kurenkov-wacv-mechsearch.html" rel="alternate" type="text/html" title="Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter" /><published>2018-09-15T00:00:00-07:00</published><updated>2018-09-15T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/09/15/kurenkov-wacv-mechsearch</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/09/15/kurenkov-wacv-mechsearch.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration</title><link href="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html" rel="alternate" type="text/html" title="Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration" /><published>2018-08-01T00:00:00-07:00</published><updated>2018-08-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html">&lt;p&gt;Our goal is for a robot to execute a previously unseen task based on a single video demonstration of the task. The success of our approach relies on the principle of transferring knowledge from seen tasks to unseen ones with similar semantics. More importantly, we hypothesize that to successfully execute a complex task from a single video demonstration, it is necessary to explicitly incorporate compositionality to the model. To test our hypothesis, we propose Neural Task Graph (NTG) Networks, which use task graph as the intermediate representation to modularize the representations of both the video demonstration and the derived policy. We show this formulation achieves strong inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. We further show that the same principle is applicable to real-world videos. We show that NTG can improve data efficiency of few-shot activity understanding in the Breakfast Dataset.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision</title><link href="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html" rel="alternate" type="text/html" title="Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision" /><published>2018-06-26T00:00:00-07:00</published><updated>2018-06-26T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html">&lt;p&gt;Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and thus properly grasping and manipulating the tool to achieve the task. Task-agnostic grasping optimizes for grasp robustness while ignoring crucial task-specific constraints. In this paper, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering. Supplementary material is available at: bit.ly/task-oriented-grasp&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video</title><link href="http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit.html" rel="alternate" type="text/html" title="Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video" /><published>2018-06-01T00:00:00-07:00</published><updated>2018-06-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit.html">&lt;p&gt;Grounding textual phrases in visual content with standalone image-sentence pairs is a challenging task. When we consider grounding in instructional videos, this problem becomes profoundly more complex: the latent temporal structure of instructional videos breaks independence assumptions and necessitates contextual understanding for resolving ambiguous visual-linguistic cues. Furthermore, dense annotations and video data scale mean supervised approaches are prohibitively costly. In this work, we propose to tackle this new task with a weakly-supervised framework for reference-aware visual grounding in instructional videos, where only the temporal alignment between the transcription and the video segment are available for supervision. We introduce the visually grounded action graph, a structured representation capturing the latent dependency between grounding and references in video. For optimization, we propose a new reference-aware multiple instance learning (RA-MIL) objective for weak supervision of grounding in videos. We evaluate our approach over unconstrained videos from YouCookII and RoboWatch, augmented with new reference-grounding test set annotations. We demonstrate that our jointly optimized, reference-aware approach simultaneously improves visual grounding, reference-resolution, and generalization to unseen instructional video categories.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</title><link href="http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp.html" rel="alternate" type="text/html" title="Neural Task Programming: Learning to Generalize Across Hierarchical Tasks" /><published>2018-05-21T00:00:00-07:00</published><updated>2018-05-21T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp.html">&lt;p&gt;In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well to- wards unseen tasks with increasing lengths, variable topologies, and changing objectives.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">DeformNet: Free-Form Deformation Network for 3d Shape Reconstruction From a Single Image</title><link href="http://pair.stanford.edu/publications/2018/03/08/kurenkov-wacv-deformnet.html" rel="alternate" type="text/html" title="DeformNet: Free-Form Deformation Network for 3d Shape Reconstruction From a Single Image" /><published>2018-03-08T00:00:00-08:00</published><updated>2018-03-08T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2018/03/08/kurenkov-wacv-deformnet</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/03/08/kurenkov-wacv-deformnet.html">&lt;p&gt;3D reconstruction from a single image is a key problem in multiple applications ranging from robotic manipulation to augmented reality. Prior methods have tackled this problem through generative models which predict 3D reconstructions as voxels or point clouds. However, these methods can be computationally expensive and miss fine details. We introduce a new differentiable layer for 3D data deformation and use it in DeformNet to learn a model for 3D reconstruction-through-deformation. DeformNet takes an image input, searches the nearest shape template from a database, and deforms the template to match the query image. We evaluate our approach on the ShapeNet dataset and show that - (a) the Free-Form Deformation layer is a powerful new building block for Deep Learning models that manipulate 3D data (b) DeformNet uses this FFD layer combined with shape retrieval for smooth and detail-preserving 3D reconstruction of qualitatively plausible point clouds with respect to a single query image (c) compared to other state-of-the-art 3D reconstruction methods, DeformNet quantitatively matches or outperforms their benchmarks by significant margins.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry></feed>