<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://pair.stanford.edu/feed.xml" rel="self" type="application/atom+xml" /><link href="http://pair.stanford.edu/" rel="alternate" type="text/html" /><updated>2018-11-21T18:50:38-08:00</updated><id>http://pair.stanford.edu/</id><title type="html">Stanford PAIR Website</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal.html" rel="alternate" type="text/html" title="Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks" /><published>2018-10-25T00:00:00-07:00</published><updated>2018-10-25T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/25/lee-icra18-multimodal.html">&lt;p&gt;Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation</title><link href="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html" rel="alternate" type="text/html" title="RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html">&lt;p&gt;Imitation Learning has empowered recent advances in learning robotic manipulation tasks by addressing shortcomings of Reinforcement Learning such as exploration and reward specification. However, research in this area has been limited to modest-sized datasets due to the difficulty of collecting large quantities of task demonstrations through existing mechanisms. This work introduces ROBOTURK to address this challenge. ROBOTURK is a crowdsourcing platform for high quality 6-DoF trajectory based teleoperation through the use of widely available mobile devices (e.g. iPhone). We evaluate ROBOTURK on three manipulation tasks of varying timescales (15-120s) and observe that our user interface is statistically similar to special purpose hardware such as virtual reality controllers in terms of task completion times. Furthermore, we observe that poor network conditions, such as low bandwidth and high delay links, do not substantially affect the remote users’ ability to perform task demonstrations successfully on ROBOTURK. Lastly, we demonstrate the efficacy of ROBOTURK through the collection of a pilot dataset; using ROBOTURK, we collected 137.5 hours of manipulation data from remote workers, amounting to over 2200 successful task demonstrations in 22 hours of total system usage. We show that the data obtained through ROBOTURK enables policy learning on multi-step manipulation tasks with sparse rewards and that using larger quantities of demonstrations during policy learning provides benefits in terms of both learning consistency and final performance.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark</title><link href="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html" rel="alternate" type="text/html" title="SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html">&lt;p&gt;Reproducibility has been a significant challenge in deep reinforcement learning and robotics research. Open-source frameworks and standardized benchmarks can serve an integral role in rigorous evaluation and reproducible research. We introduce SURREAL, an open-source scalable framework that supports stateof-the-art distributed reinforcement learning algorithms. We design a principled distributed learning formulation that accommodates both on-policy and off-policy learning. We demonstrate that SURREAL algorithms outperform existing open-source implementations in both agent performance and learning efficiency. We also introduce SURREAL Robotics Suite, an accessible set of benchmarking tasks in physical simulation for reproducible robot manipulation research. We provide extensive evaluations of SURREAL algorithms and establish strong baseline results.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration</title><link href="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html" rel="alternate" type="text/html" title="Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration" /><published>2018-08-01T00:00:00-07:00</published><updated>2018-08-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html">&lt;p&gt;Our goal is for a robot to execute a previously unseen task based on a single video demonstration of the task. The success of our approach relies on the principle of transferring knowledge from seen tasks to unseen ones with similar semantics. More importantly, we hypothesize that to successfully execute a complex task from a single video demonstration, it is necessary to explicitly incorporate compositionality to the model. To test our hypothesis, we propose Neural Task Graph (NTG) Networks, which use task graph as the intermediate representation to modularize the representations of both the video demonstration and the derived policy. We show this formulation achieves strong inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. We further show that the same principle is applicable to real-world videos. We show that NTG can improve data efficiency of few-shot activity understanding in the Breakfast Dataset.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision</title><link href="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html" rel="alternate" type="text/html" title="Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision" /><published>2018-06-26T00:00:00-07:00</published><updated>2018-06-26T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html">&lt;p&gt;Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and thus properly grasping and manipulating the tool to achieve the task. Task-agnostic grasping optimizes for grasp robustness while ignoring crucial task-specific constraints. In this paper, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering. Supplementary material is available at: bit.ly/task-oriented-grasp&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video</title><link href="http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit.html" rel="alternate" type="text/html" title="Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video" /><published>2018-06-01T00:00:00-07:00</published><updated>2018-06-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/06/01/huang-cvpr18-findingit.html">&lt;p&gt;Grounding textual phrases in visual content with standalone image-sentence pairs is a challenging task. When we consider grounding in instructional videos, this problem becomes profoundly more complex: the latent temporal structure of instructional videos breaks independence assumptions and necessitates contextual understanding for resolving ambiguous visual-linguistic cues. Furthermore, dense annotations and video data scale mean supervised approaches are prohibitively costly. In this work, we propose to tackle this new task with a weakly-supervised framework for reference-aware visual grounding in instructional videos, where only the temporal alignment between the transcription and the video segment are available for supervision. We introduce the visually grounded action graph, a structured representation capturing the latent dependency between grounding and references in video. For optimization, we propose a new reference-aware multiple instance learning (RA-MIL) objective for weak supervision of grounding in videos. We evaluate our approach over unconstrained videos from YouCookII and RoboWatch, augmented with new reference-grounding test set annotations. We demonstrate that our jointly optimized, reference-aware approach simultaneously improves visual grounding, reference-resolution, and generalization to unseen instructional video categories.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</title><link href="http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp.html" rel="alternate" type="text/html" title="Neural Task Programming: Learning to Generalize Across Hierarchical Tasks" /><published>2018-05-21T00:00:00-07:00</published><updated>2018-05-21T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/05/21/xu-icra18-ntp.html">&lt;p&gt;In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well to- wards unseen tasks with increasing lengths, variable topologies, and changing objectives.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">AdaPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems</title><link href="http://pair.stanford.edu/publications/2017/12/01/harrison-isrr17-adapt.html" rel="alternate" type="text/html" title="AdaPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems" /><published>2017-12-01T00:00:00-08:00</published><updated>2017-12-01T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2017/12/01/harrison-isrr17-adapt</id><content type="html" xml:base="http://pair.stanford.edu/publications/2017/12/01/harrison-isrr17-adapt.html">&lt;p&gt;Model-free policy learning has enabled robust performance of complex tasks with relatively simple algorithms. However, this simplicity comes at the cost of requiring an Oracle and arguably very poor sample complexity. This renders such methods unsuitable for physical systems. Variants of model-based methods address this problem through the use of simulators, however, this gives rise to the problem of policy transfer from simulated to the physical system. Model mismatch due to systematic parameter shift and unmodelled dynamics error may cause sub-optimal or unsafe behavior upon direct transfer. We introduce the Adaptive Policy Transfer for Stochastic Dynamics (ADAPT) algorithm that achieves provably safe and robust, dynamically-feasible zero-shot transfer of RL-policies to new domains with dynamics error. ADAPT combines the strengths of offline policy learning in a black-box source simulator with online tube-based MPC to attenuate bounded model mismatch between the source and target dynamics. ADAPT allows online transfer of policy, trained solely in a simulation offline, to a family of unknown targets without fine-tuning. We also formally show that (i) ADAPT guarantees state and control safety through state-action tubes under the assumption of Lipschitz continuity of the divergence in dynamics and, (ii) ADAPT results in a bounded loss of reward accumulation relative to a policy trained and evaluated in the source environment. We evaluate ADAPT on 2 continuous, non-holonomic simulated dynamical systems with 4 different disturbance models, and find that ADAPT performs between 50%-300% better on mean reward accrual than direct policy transfer.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Visual Semantic Planning using Deep Successor Representations</title><link href="http://pair.stanford.edu/publications/2017/10/01/zhu-iccv17-vsp.html" rel="alternate" type="text/html" title="Visual Semantic Planning using Deep Successor Representations" /><published>2017-10-01T00:00:00-07:00</published><updated>2017-10-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2017/10/01/zhu-iccv17-vsp</id><content type="html" xml:base="http://pair.stanford.edu/publications/2017/10/01/zhu-iccv17-vsp.html">&lt;p&gt;A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Adversarially Robust Policy Learning through Active Construction of Physically-Plausible Perturbations</title><link href="http://pair.stanford.edu/publications/2017/10/01/mandlekar-iros17-arpl.html" rel="alternate" type="text/html" title="Adversarially Robust Policy Learning through Active Construction of Physically-Plausible Perturbations" /><published>2017-10-01T00:00:00-07:00</published><updated>2017-10-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2017/10/01/mandlekar-iros17-arpl</id><content type="html" xml:base="http://pair.stanford.edu/publications/2017/10/01/mandlekar-iros17-arpl.html">&lt;p&gt;Policy search methods in reinforcement learning have demonstrated success in scaling up to larger problems beyond toy examples. However, deploying these methods on real robots remains challenging due to the large sample complexity required during learning and their vulnerability to malicious intervention. We introduce Adversarially Robust Policy Learning (ARPL), an algorithm that leverages active computation of physically-plausible adversarial examples during training to enable robust policy learning in the source domain and robust performance under both random and adversarial input perturbations. We evaluate ARPL on four continuous control tasks and show superior resilience to changes in physical environment dynamics parameters and environment state as compared to state-of-the-art robust policy learning methods.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry></feed>