<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://pair.stanford.edu/feed.xml" rel="self" type="application/atom+xml" /><link href="http://pair.stanford.edu/" rel="alternate" type="text/html" /><updated>2019-11-20T21:16:18-08:00</updated><id>http://pair.stanford.edu/</id><title type="html">Stanford PAIR Website</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity</title><link href="http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling.html" rel="alternate" type="text/html" title="Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity" /><published>2019-11-10T00:00:00-08:00</published><updated>2019-11-10T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning</title><link href="http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing.html" rel="alternate" type="text/html" title="Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning" /><published>2019-11-09T00:00:00-08:00</published><updated>2019-11-09T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Variable Impedance in End-Effector Space: An Action Space for Reinforcement Learning of Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable.html" rel="alternate" type="text/html" title="Variable Impedance in End-Effector Space: An Action Space for Reinforcement Learning of Contact-Rich Tasks" /><published>2019-11-08T00:00:00-08:00</published><updated>2019-11-08T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers</title><link href="http://pair.stanford.edu/publications/2019/10/30/kurenkov-corl19-acteach.html" rel="alternate" type="text/html" title="AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers" /><published>2019-10-30T00:00:00-07:00</published><updated>2019-10-30T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/10/30/kurenkov-corl19-acteach</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/10/30/kurenkov-corl19-acteach.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation</title><link href="http://pair.stanford.edu/publications/2019/10/29/fang-corl19-cascade.html" rel="alternate" type="text/html" title="Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation" /><published>2019-10-29T00:00:00-07:00</published><updated>2019-10-29T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/10/29/fang-corl19-cascade</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/10/29/fang-corl19-cascade.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Situational Fusion of Visual Representation for Visual Navigation</title><link href="http://pair.stanford.edu/publications/2019/10/27/shen-iccv19-fusion.html" rel="alternate" type="text/html" title="Situational Fusion of Visual Representation for Visual Navigation" /><published>2019-10-27T00:00:00-07:00</published><updated>2019-10-27T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/10/27/shen-iccv19-fusion</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/10/27/shen-iccv19-fusion.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision</title><link href="http://pair.stanford.edu/publications/2019/08/01/fang-ijrr19-tog.html" rel="alternate" type="text/html" title="Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision" /><published>2019-08-01T00:00:00-07:00</published><updated>2019-08-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/08/01/fang-ijrr19-tog</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/08/01/fang-ijrr19-tog.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title><link href="http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose.html" rel="alternate" type="text/html" title="DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion" /><published>2019-06-19T00:00:00-07:00</published><updated>2019-06-19T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal.html" rel="alternate" type="text/html" title="Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks" /><published>2019-05-16T00:00:00-07:00</published><updated>2019-05-16T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal.html">&lt;p&gt;Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter</title><link href="http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch.html" rel="alternate" type="text/html" title="Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter" /><published>2019-05-15T00:00:00-07:00</published><updated>2019-05-15T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch.html"></content><author><name></name></author><summary type="html">text text text</summary></entry></feed>