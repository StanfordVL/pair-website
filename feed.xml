<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://pair.stanford.edu/feed.xml" rel="self" type="application/atom+xml" /><link href="http://pair.stanford.edu/" rel="alternate" type="text/html" /><updated>2019-06-24T08:07:52-07:00</updated><id>http://pair.stanford.edu/</id><title type="html">Stanford PAIR Website</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity</title><link href="http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling.html" rel="alternate" type="text/html" title="Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity" /><published>2019-11-10T00:00:00-08:00</published><updated>2019-11-10T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/10/mandlekar-iros19-roboturkscaling.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning</title><link href="http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing.html" rel="alternate" type="text/html" title="Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning" /><published>2019-11-09T00:00:00-08:00</published><updated>2019-11-09T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/09/huang-iros19-relaxing.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Variable Impedance in End-Effector Space: An Action Space for Reinforcement Learning of Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable.html" rel="alternate" type="text/html" title="Variable Impedance in End-Effector Space: An Action Space for Reinforcement Learning of Contact-Rich Tasks" /><published>2019-11-08T00:00:00-08:00</published><updated>2019-11-08T00:00:00-08:00</updated><id>http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/11/08/martin-iros19-variable.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion</title><link href="http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose.html" rel="alternate" type="text/html" title="DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion" /><published>2019-06-19T00:00:00-07:00</published><updated>2019-06-19T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/06/19/wang-cvpr19-6dofpose.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks</title><link href="http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal.html" rel="alternate" type="text/html" title="Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks" /><published>2019-05-16T00:00:00-07:00</published><updated>2019-05-16T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/05/16/lee-icra19-multimodal.html">&lt;p&gt;Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter</title><link href="http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch.html" rel="alternate" type="text/html" title="Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter" /><published>2019-05-15T00:00:00-07:00</published><updated>2019-05-15T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch</id><content type="html" xml:base="http://pair.stanford.edu/publications/2019/05/15/kurenkov-icra19-mechsearch.html"></content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation</title><link href="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html" rel="alternate" type="text/html" title="RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/mandlekar-corl18-roboturk.html">&lt;p&gt;Imitation Learning has empowered recent advances in learning robotic manipulation tasks by addressing shortcomings of Reinforcement Learning such as exploration and reward specification. However, research in this area has been limited to modest-sized datasets due to the difficulty of collecting large quantities of task demonstrations through existing mechanisms. This work introduces ROBOTURK to address this challenge. ROBOTURK is a crowdsourcing platform for high quality 6-DoF trajectory based teleoperation through the use of widely available mobile devices (e.g. iPhone). We evaluate ROBOTURK on three manipulation tasks of varying timescales (15-120s) and observe that our user interface is statistically similar to special purpose hardware such as virtual reality controllers in terms of task completion times. Furthermore, we observe that poor network conditions, such as low bandwidth and high delay links, do not substantially affect the remote usersâ€™ ability to perform task demonstrations successfully on ROBOTURK. Lastly, we demonstrate the efficacy of ROBOTURK through the collection of a pilot dataset; using ROBOTURK, we collected 137.5 hours of manipulation data from remote workers, amounting to over 2200 successful task demonstrations in 22 hours of total system usage. We show that the data obtained through ROBOTURK enables policy learning on multi-step manipulation tasks with sparse rewards and that using larger quantities of demonstrations during policy learning provides benefits in terms of both learning consistency and final performance.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark</title><link href="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html" rel="alternate" type="text/html" title="SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark" /><published>2018-10-24T00:00:00-07:00</published><updated>2018-10-24T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/10/24/fan-corl18-surreal.html">&lt;p&gt;Reproducibility has been a significant challenge in deep reinforcement learning and robotics research. Open-source frameworks and standardized benchmarks can serve an integral role in rigorous evaluation and reproducible research. We introduce SURREAL, an open-source scalable framework that supports stateof-the-art distributed reinforcement learning algorithms. We design a principled distributed learning formulation that accommodates both on-policy and off-policy learning. We demonstrate that SURREAL algorithms outperform existing open-source implementations in both agent performance and learning efficiency. We also introduce SURREAL Robotics Suite, an accessible set of benchmarking tasks in physical simulation for reproducible robot manipulation research. We provide extensive evaluations of SURREAL algorithms and establish strong baseline results.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration</title><link href="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html" rel="alternate" type="text/html" title="Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration" /><published>2018-08-01T00:00:00-07:00</published><updated>2018-08-01T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/08/01/huang-arxiv18-ntg.html">&lt;p&gt;Our goal is for a robot to execute a previously unseen task based on a single video demonstration of the task. The success of our approach relies on the principle of transferring knowledge from seen tasks to unseen ones with similar semantics. More importantly, we hypothesize that to successfully execute a complex task from a single video demonstration, it is necessary to explicitly incorporate compositionality to the model. To test our hypothesis, we propose Neural Task Graph (NTG) Networks, which use task graph as the intermediate representation to modularize the representations of both the video demonstration and the derived policy. We show this formulation achieves strong inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. We further show that the same principle is applicable to real-world videos. We show that NTG can improve data efficiency of few-shot activity understanding in the Breakfast Dataset.&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry><entry><title type="html">Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision</title><link href="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html" rel="alternate" type="text/html" title="Learning Task-Oriented Grasping for Tool Manipulation with Simulated Self-Supervision" /><published>2018-06-26T00:00:00-07:00</published><updated>2018-06-26T00:00:00-07:00</updated><id>http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog</id><content type="html" xml:base="http://pair.stanford.edu/publications/2018/06/26/fang-rss18-tog.html">&lt;p&gt;Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and thus properly grasping and manipulating the tool to achieve the task. Task-agnostic grasping optimizes for grasp robustness while ignoring crucial task-specific constraints. In this paper, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering. Supplementary material is available at: bit.ly/task-oriented-grasp&lt;/p&gt;</content><author><name></name></author><summary type="html">text text text</summary></entry></feed>